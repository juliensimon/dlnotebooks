{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar words and analogies with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU gluonnlp mxnet awscli botocore boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd      # NDArray API\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity of two vectors is the normalized dot product of the two vectors. This value is actually the cosinus of the angle between the two vectors, hence its name!\n",
    "\n",
    "  * colinear vectors --> close to +1 : words are used in the same context\n",
    "  * opposite vectors --> close to -1 : words are used in a different context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"cosine.png\")\n",
    "# Source: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost co-linear: b ≈ 2a \n",
    "a = mx.nd.array([1, 2])\n",
    "b = mx.nd.array([2.1, 3.9])\n",
    "print(mx.nd.dot(a, b) / (a.norm() * b.norm()))\n",
    "\n",
    "# Almost opposite: b ≈ -2a\n",
    "a = mx.nd.array([1, 2])\n",
    "b = mx.nd.array([-2.1, -3.9])\n",
    "print(mx.nd.dot(a, b) / (a.norm() * b.norm()))\n",
    "\n",
    "# Almost orthogonal\n",
    "a = mx.nd.array([1, 2])\n",
    "b = mx.nd.array([2.1, -0.9])\n",
    "print(mx.nd.dot(a, b) / (a.norm() * b.norm()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that transforms two words into their embeddings, and compute their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(embedding, word1, word2):\n",
    "    vec1, vec2 = embedding[word1], embedding[word2]\n",
    "    return mx.nd.dot(vec1, vec2) / (vec1.norm() * vec2.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove is a popular algorithm for word embeddings. Let's see which pre-trained embeddings are available in GluonNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.embedding.list_sources('glove')     # we could also use 'fasttext' and 'word2vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download embeddings built from a 6-billion word corpus encoded in 50-dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = nlp.embedding.create('glove', source='glove.6B.50d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the vocabulary for that corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(nlp.data.Counter(glove.idx_to_token))\n",
    "vocab.set_embedding(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the two words we'd like to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'burger'\n",
    "word2 = 'fries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.embedding[word1])\n",
    "print(vocab.embedding[word2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hard to see in 50 dimenstions! Let's compute their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Similarity:', cos_similarity(glove, word1, word2).asnumpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding words that have a similar meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a simple way to compute the dot product of a given embedding with respect to all other embeddings, and select the top values.\n",
    "\n",
    "First, we need to normalize all embeddings, to make sure all dot products will be in the [-1, +1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / nd.sqrt(nd.sum(x * x, axis=1) + 1E-10).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Intuitively, two embeddings for similar words will now point at the same location. Remember we're working with 50 dimensions... Here's an example with only 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"normalized vectors.png\")\n",
    "# Source: \"Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation\", Xing et al, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the dot products for a given word, and return the top 'k' ones. We use the NDArray API from Apache MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_words(vocab, k, word):\n",
    "    word_vec = vocab.embedding[word].reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+1, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # Remove unknown and input tokens.\n",
    "    return vocab.to_tokens(indices[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_words(vocab, 5, 'burger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could do the opposite, and pick the 'k' smallest values to find unrelated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_furthest_words(vocab, k, word):\n",
    "    word_vec = vocab.embedding[word].reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+1, ret_typ='indices', is_ascend=True)\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # Remove unknown and input tokens.\n",
    "    return vocab.to_tokens(indices[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_furthest_words(vocab, 5, 'burger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distance between vector1 and vector2 is close to the distance between vector3 and vector4, then (word1, word2) and (word3, word4) illustrate a similar relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes three words as input: it literally computes vector2 - vector1 + vector3, computes the dot product of the resulting vector with all other embeddings, and return the top 'k' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_by_analogy(vocab, k, word1, word2, word3):\n",
    "    word_vecs = vocab.embedding[word1, word2, word3]\n",
    "    word_diff = (word_vecs[1] - word_vecs[0] + word_vecs[2]).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_diff)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    return vocab.to_tokens(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Cold' is to 'colder' what 'warm' is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'cold', 'colder', 'warm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'King' is to 'man' what 'queen' is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'king', 'man', 'queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Cars' is to 'ferrari' what 'jewelry' is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'cars', 'ferrari', 'jewelry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
