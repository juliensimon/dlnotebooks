{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning with Keras on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker is a modular, fully managed Machine Learning service that lets you easily build, train and deploy models at any scale.\n",
    "\n",
    "In this workshop, we'll use Keras (with the TensorFlow backend) to build a simple Convolutional Neural Network (CNN). We'll then train it to classify the Fashion-MNIST image data set. Fashion-MNIST is a Zalando dataset consisting of a training set of 60,000 examples and a validation set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes: it's a drop-in replacement for MNIST.\n",
    "\n",
    "First, you will learn how to:\n",
    "  * use the SageMaker SDK\n",
    "  * adapt existing code to run on SageMaker ('script mode')\n",
    "  * train your code locally for fast experimentation, without firing up managed infrastructure ('local mode')\n",
    "  * train and deploy models on managed infrastructure\n",
    "  * predict data samples\n",
    "  \n",
    "Then, we'll more to more advanced topics, such as:\n",
    "  * saving up to 80% on training costs with Managed Spot Training\n",
    "  * saving up to 80% on GPU inference costs with Elastic Inference \n",
    "  * finding optimal hyper parameters automatically with Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "  * Amazon SageMaker documentation [ https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html ]\n",
    "  * SageMaker SDK \n",
    "    * Code [ https://github.com/aws/sagemaker-python-sdk ] \n",
    "    * Documentation [ https://sagemaker.readthedocs.io/ ]\n",
    "  * Fashion-MNIST [ https://github.com/zalandoresearch/fashion-mnist ] \n",
    "  * Keras documentation [ https://keras.io/ ]\n",
    "  * Numpy documentation [ https://docs.scipy.org/doc/numpy/index.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the latest SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the Jupyter kernel to use the latest SageMaker SDK (\"Kernel\" / \"Restart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"fashion-mnist-sprite.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data set from the Internet. Fortunately, Keras provides a simple way to do this. The data set is already split (training and validation), with separate Numpy arrays for samples and labels. \n",
    "\n",
    "We create a local directory, and save the training and validation data sets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "\n",
    "np.savez('./data/training', image=x_train, label=y_train)\n",
    "np.savez('./data/validation', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at our Keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pygmentize mnist_keras_tf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main steps are:\n",
    "  * receive and parse command line arguments: five hyper parameters, and four environment variables (we'll get back to these in a moment)\n",
    "  * load the data sets\n",
    "  * make sure data sets have the right shape for TensorFlow (channels last)\n",
    "  * normalize data sets, i.e. tranform [0-255] pixel values to [0-1] values\n",
    "  * one-hot encode category labels (not familiar with this? More info: [ https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/ ])\n",
    "  * Build a Sequential model in Keras: two convolution block with max pooling, followed by a fully connected layer with dropout, and a final classification layer. Don't worry if this sounds like gibberish, it's not our focus today\n",
    "  * Train the model, leveraging multiple GPUs if they're available.\n",
    "  * Print statistics\n",
    "  * Save the model in TensorFlow serving format\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train outside of SageMaker (just like on your laptop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training on SageMaker, let's run this code locally and make sure that it trains fine. We just need to set the four environment variables that it expects, and run the script with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set the four environment variables expected by the script\n",
    "\n",
    "# Number of GPUs on this machine\n",
    "%env SM_NUM_GPUS=\n",
    "# Where to save the model\n",
    "%env SM_MODEL_DIR=\n",
    "# Where the training data is\n",
    "%env SM_CHANNEL_TRAINING=\n",
    "# Where the validation data is\n",
    "%env SM_CHANNEL_VALIDATION=\n",
    "\n",
    "!python mnist_keras_tf.py --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are these environment variables important anyway? Well, they will be automatically passed to our script by SageMaker, so that we know where the data sets are, where to save the model, and how many GPUs we have. So, if you write your local code this way, **there won't be anything to change** to run it on SageMaker.\n",
    "\n",
    "This feature is called '**script mode**', it's the recommended way to work with built-in frameworks on SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on the notebook instance (aka 'local mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code runs fine. Now, let's try to run it inside the built-in TensorFlow environment provided by SageMaker. For fast experimentation, let's use local\n",
    "\n",
    "Read this first, and complete the cells below: [ https://sagemaker.readthedocs.io/en/stable/using_tf.html ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "## TODO: configure a local training job for 'mnist_keras_tf.py', \n",
    "#        training with TensorFlow 1.14 in script mode for just one epoch\n",
    "\n",
    "tf_estimator = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the local location of the training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: define the local location for the training and validation data sets\n",
    "\n",
    "local_training_input_path   = # TODO\n",
    "local_validation_input_path = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train on the local training and validation data sets\n",
    "\n",
    "tf_estimator.fit(# TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, our job runs fine locally. Let's now run the same job on a managed instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data set to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker training instances expect data sets to be stored in Amazon S3, so let's upload them there. We could use boto3 to do this, but the SageMaker SDK includes a simple function: *Session.upload_data()*\n",
    "\n",
    "[https://sagemaker.readthedocs.io/en/stable/session.html]\n",
    "\n",
    "*Note: for high-performance workloads, Amazon EFS and Amazon FSx for Lustre are now also supported. More info here: [ https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/ ]*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'keras-fashion-mnist'\n",
    "\n",
    "# TODO: in the default bucket, upload the training data set to 'keras-fashion-mnist/training'\n",
    "training_input_path   = # TODO\n",
    "\n",
    "# TODO: in the default bucket, upload the validation data set to 'keras-fashion-mnist/validation'\n",
    "validation_input_path = # TODO\n",
    "\n",
    "print(training_input_path)\n",
    "print(validation_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with our data set. Of course, in real life, much more work would be needed for data cleaning and preparation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job on a fully managed instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: configure a managed training job for 'mnist_keras_tf.py', \n",
    "#        using a single c5.2xlarge instance\n",
    "#        running TensorFlow 1.14 in script mode for ten epochs\n",
    "\n",
    "tf_estimator = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train on the training and validation data sets stored in S3\n",
    "\n",
    "tf_estimator.fit(# TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take 4-5 minutes. Please take a look at the training log. The first few lines show SageMaker preparing the managed instance.\n",
    "\n",
    "While the job is training, you can also look at metrics in the AWS console for SageMaker, and at the training log in the the AWS console for CloudWatch Logs.\n",
    "\n",
    "Once the job is complete, the trained model is saved in S3, and is now ready to be deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our model to a real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Deploy the model to an endpoint backed by a single m5.large instance\n",
    "\n",
    "import time\n",
    "\n",
    "tf_endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "tf_predictor = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take about 7-8 minutes. While the model is deploying, please take a look at the \"Endpoints\" section in the AWS console for SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, we can use it to predict data samples.\n",
    "\n",
    "The cell below grabs 10 random images from the validation data set, and uses the predict() API to predict their class. \n",
    "\n",
    "More precisely, we build a prediction request in TensorFlow Serving format, and send an HTTPS POST request to the prediction endpoint: its URL is visible in the AWS console for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10\n",
    "indices = random.sample(range(x_val.shape[0] - 1), num_samples)\n",
    "images = x_val[indices]/255\n",
    "labels = y_val[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "prediction = tf_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "prediction = np.array(prediction)\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('Predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're done with this endpoint, we should delete it to avoid unecessary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered the basics. Now let's look at more advanced topics.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managed Spot Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EC2 Spot Instances have long been a great cost optimization feature, and spot training is now available on SageMaker.\n",
    "\n",
    "This blog post has more info: [ https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: configure the same training job as above,\n",
    "#        with Managed Spot Training\n",
    "\n",
    "tf_estimator = # TODO\n",
    "\n",
    "tf_estimator.fit(# TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the end of the training log. How much did you save?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Inference is a feature that lets you attach fractional GPU acceleration to any EC2 instance. It's also available on SageMaker.\n",
    "\n",
    "This blog post has more info: [ https://aws.amazon.com/blogs/aws/amazon-elastic-inference-gpu-powered-deep-learning-inference-acceleration/ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Deploy the model to an endpoint backed by a single c5.large instance,\n",
    "#  accelerated by a medium-size elastic inference accelerator\n",
    "\n",
    "tf_endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "tf_predictor = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is deployed, you can use the 'Predict' cell above to send it some samples.\n",
    "\n",
    "Don't forget to delete the endpoint once you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic model tuning is a great feature that helps you find automatically the best hyper parameters for your training job.\n",
    "\n",
    "This blog post has more info: [ https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define parameter ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: define parameter ranges like so:\n",
    "#       - learning rate: from 0.001 to 0.1, with logarithmic scaling\n",
    "#       - batch size: from 32 to 1024\n",
    "#       - dense layer: from 128 to 1024 neurons\n",
    "#       - dropout: from 0.2 to 0.6\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    # TODO\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the metric we're optimizing for, in this case we want to maximize validation accuracy. This value is reported as 'val_acc' in the training log, and we have to pass a regular expression so that SageMaker can find it accuratelmy\n",
    "\n",
    "This last step is not necessary for built-in algorithms and built-in frameworks, we can simply pass the metric name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'val_acc'\n",
    "objective_type = 'Maximize'\n",
    "metric_definitions = [{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it's time to put everything together, and configure the tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "## TODO: configure a training job using the Tensorflow estimator, the parameter ranges and the metric defined above.\n",
    "#        Let's run ten individual jobs, two by two.\n",
    "\n",
    "tuner = HyperparameterTuner(# TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the tuning job, just like a normal estimator. We definitely want to use spot training here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: launch the tuning job, passing the location of the data sets in S3.\n",
    "\n",
    "tuner.fit(# TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the job is running, you can view it in the AWS console for SageMaker: individual jobs (and their logs), best training job so far, etc.\n",
    "\n",
    "Of course, you can also inspect the job programatically using boto3 : *decribe_hyper_parameter_training_job()*, etc. [ https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the tuning job is complete, you can deploy the best model, just like we did for a normal estimator.\n",
    "\n",
    "*Note: if you call deploy() while the tuning job is running, it will deploy the best current model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "## TODO: Deploy the model to an endpoint backed by a single c5.large instance,\n",
    "#  accelerated by a medium-size elastic inference accelerator\n",
    "\n",
    "tf_predictor = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can use the 'Predict' cell above to send some samples to the endpoint.\n",
    "\n",
    "Don't forget to delete the endpoint once you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've made it to the end of this workshop. I hope that you had fun and learned a lot! Before leaving, please check the AWS console for SageMaker, and check that you've shutdown all your endpoints in order to avoid unecessary costs.\n",
    "\n",
    "If you want to learn more, please check out the large collection of SageMaker notebooks on Github:\n",
    "[ https://github.com/awslabs/amazon-sagemaker-examples ]\n",
    "\n",
    "Julien Simon @julsimon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
