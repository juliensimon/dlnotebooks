{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, ndarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/.mxnet/datasets/mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading /home/ec2-user/.mxnet/datasets/mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading /home/ec2-user/.mxnet/datasets/mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading /home/ec2-user/.mxnet/datasets/mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    }
   ],
   "source": [
    "# Download the MNIST dataset, then create the training and test sets \n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=lambda data, label: (data.astype(np.float32)/255, label)),\n",
    "                                      batch_size=32, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=lambda data, label: (data.astype(np.float32)/255, label)),\n",
    "                                     batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "net = gluon.nn.Sequential()\n",
    "\n",
    "# Define the model architecture\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(128, activation=\"relu\")) # 1st layer - 128 nodes\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\")) # 2nd layer – 64 nodes\n",
    "    net.add(gluon.nn.Dense(10)) # Output layer, one for each number 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with random values for all of the model’s parameters from a \n",
    "# normal distribution with a standard deviation of 0.05\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=0.05))\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# We opt to use the stochastic gradient descent (sgd) training algorithm \n",
    "# and set the learning rate hyperparameter to .1\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current Loss: 0.04569364711642265.\n",
      "Epoch 1. Current Loss: 0.007011435925960541.\n",
      "Epoch 2. Current Loss: 0.01731630228459835.\n",
      "Epoch 3. Current Loss: 0.050766248255968094.\n",
      "Epoch 4. Current Loss: 0.00788209494203329.\n",
      "Epoch 5. Current Loss: 0.13930395245552063.\n",
      "Epoch 6. Current Loss: 0.017785532400012016.\n",
      "Epoch 7. Current Loss: 0.009234237484633923.\n",
      "Epoch 8. Current Loss: 0.017325488850474358.\n",
      "Epoch 9. Current Loss: 0.012752940878272057.\n"
     ]
    }
   ],
   "source": [
    "# Loop through several epochs and watch the model improve\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(mx.cpu()).reshape((-1, 784))\n",
    "        label = label.as_in_context(mx.cpu())\n",
    "        with autograd.record(): # Start recording the derivatives\n",
    "            output = net(data) # the forward iteration\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        # Provide stats on the improvement of the model over each epoch\n",
    "        curr_loss = ndarray.mean(loss).asscalar()\n",
    "    print(\"Epoch {}. Current Loss: {}.\".format(e, curr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
