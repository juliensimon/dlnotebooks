{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd\n",
    "from inception_score import get_inception_score\n",
    "\n",
    "mpl.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_buf(buf, i, img, shape):\n",
    "    \"\"\"Reposition the images generated by the generator so that it can be saved as picture matrix.\n",
    "    :param buf: the images metric\n",
    "    :param i: index of each image\n",
    "    :param img: images generated by generator once\n",
    "    :param shape: each image`s shape\n",
    "    :return: Adjust images for output\n",
    "    \"\"\"\n",
    "    n = buf.shape[0]//shape[1]\n",
    "    m = buf.shape[1]//shape[0]\n",
    "\n",
    "    sx = (i%m)*shape[0]\n",
    "    sy = (i//m)*shape[1]\n",
    "    buf[sy:sy+shape[1], sx:sx+shape[0], :] = img\n",
    "\n",
    "\n",
    "def visual(title, X, name):\n",
    "    \"\"\"Image visualization and preservation\n",
    "    :param title: title\n",
    "    :param X: images to visualized\n",
    "    :param name: saved picture`s name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(X.shape) == 4\n",
    "    X = X.transpose((0, 2, 3, 1))\n",
    "    X = np.clip((X - np.min(X))*(255.0/(np.max(X) - np.min(X))), 0, 255).astype(np.uint8)\n",
    "    n = np.ceil(np.sqrt(X.shape[0]))\n",
    "    buff = np.zeros((int(n*X.shape[1]), int(n*X.shape[2]), int(X.shape[3])), dtype=np.uint8)\n",
    "    for i, img in enumerate(X):\n",
    "        fill_buf(buff, i, img, X.shape[1:3])\n",
    "    buff = buff[:, :, ::-1]\n",
    "    plt.imshow(buff)\n",
    "    plt.title(title)\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# size of the latent z vector\n",
    "nz = 100\n",
    "# the channel of each generator filter layer\n",
    "ngf = 64\n",
    "# the channel of each discriminator filter layer\n",
    "ndf = 64\n",
    "# save generated images and inception_score per niter iters\n",
    "niter = 50\n",
    "# number of image channels\n",
    "nc = 3\n",
    "ctx = mx.cpu(0)\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "batch_size = 64\n",
    "nepoch = 25\n",
    "\n",
    "check_point = False\n",
    "#dataset = 'mnist'\n",
    "dataset = 'fashionmnist'\n",
    "#dataset = 'cifar10'\n",
    "\n",
    "outf = './results-fmnist'\n",
    "if not os.path.exists(outf):\n",
    "    os.makedirs(outf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(data, label):\n",
    "    \"\"\"Get the translation of images\"\"\"\n",
    "    # resize to 64x64\n",
    "    data = mx.image.imresize(data, 64, 64)\n",
    "    # transpose from (64, 64, 3) to (3, 64, 64)\n",
    "    data = mx.nd.transpose(data, (2, 0, 1))\n",
    "    # normalize to [-1, 1]\n",
    "    data = data.astype(np.float32)/128 - 1\n",
    "    # if image is greyscale, repeat 3 times to get RGB image.\n",
    "    if data.shape[0] == 1:\n",
    "        data = mx.nd.tile(data, (3, 1, 1))\n",
    "    return data, label\n",
    "\n",
    "# get dataset with the batch_size num each time\n",
    "def get_dataset(dataset_name):\n",
    "    \"\"\"Load the dataset and split it to train/valid data\n",
    "    :param dataset_name: string\n",
    "    Returns:\n",
    "    train_data: int array\n",
    "        training dataset\n",
    "    val_data: int array\n",
    "        valid dataset\n",
    "    \"\"\"\n",
    "    # mnist\n",
    "    if dataset == \"mnist\":\n",
    "        train_data = gluon.data.DataLoader(\n",
    "            gluon.data.vision.MNIST('./data', train=True, transform=transformer),\n",
    "            batch_size, shuffle=True, last_batch='discard')\n",
    "\n",
    "        val_data = gluon.data.DataLoader(\n",
    "            gluon.data.vision.MNIST('./data', train=False, transform=transformer),\n",
    "            batch_size, shuffle=False)\n",
    "    # cifar10\n",
    "    # mnist\n",
    "    elif dataset == \"fashionmnist\":\n",
    "        train_data = gluon.data.DataLoader(\n",
    "            gluon.data.vision.FashionMNIST('./data', train=True, transform=transformer),\n",
    "            batch_size, shuffle=True, last_batch='discard')\n",
    "\n",
    "        val_data = gluon.data.DataLoader(\n",
    "            gluon.data.vision.FashionMNIST('./data', train=False, transform=transformer),\n",
    "            batch_size, shuffle=False)\n",
    "    # cifar10\n",
    "    elif dataset == \"cifar10\":\n",
    "        train_data = gluon.data.DataLoader(\n",
    "            gluon.data.vision.CIFAR10('./data', train=True, transform=transformer),\n",
    "            batch_size, shuffle=True, last_batch='discard')\n",
    "\n",
    "        val_data = gluon.data.DataLoader(\n",
    "            gluon.data.vision.CIFAR10('./data', train=False, transform=transformer),\n",
    "            batch_size, shuffle=False)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_netG():\n",
    "    \"\"\"Get net G\"\"\"\n",
    "    # build the generator\n",
    "    netG = nn.Sequential()\n",
    "    with netG.name_scope():\n",
    "        # input is Z, going into a convolution\n",
    "        netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))\n",
    "        netG.add(nn.BatchNorm())\n",
    "        netG.add(nn.Activation('relu'))\n",
    "        # state size. (ngf*8) x 4 x 4\n",
    "        netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))\n",
    "        netG.add(nn.BatchNorm())\n",
    "        netG.add(nn.Activation('relu'))\n",
    "        # state size. (ngf*4) x 8 x 8\n",
    "        netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))\n",
    "        netG.add(nn.BatchNorm())\n",
    "        netG.add(nn.Activation('relu'))\n",
    "        # state size. (ngf*2) x 16 x 16\n",
    "        netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))\n",
    "        netG.add(nn.BatchNorm())\n",
    "        netG.add(nn.Activation('relu'))\n",
    "        # state size. (ngf) x 32 x 32\n",
    "        netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))\n",
    "        netG.add(nn.Activation('tanh'))\n",
    "        # state size. (nc) x 64 x 64\n",
    "\n",
    "    return netG\n",
    "\n",
    "\n",
    "def get_netD():\n",
    "    \"\"\"Get the netD\"\"\"\n",
    "    # build the discriminator\n",
    "    netD = nn.Sequential()\n",
    "    with netD.name_scope():\n",
    "        # input is (nc) x 64 x 64\n",
    "        netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))\n",
    "        netD.add(nn.LeakyReLU(0.2))\n",
    "        # state size. (ndf) x 32 x 32\n",
    "        netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))\n",
    "        netD.add(nn.BatchNorm())\n",
    "        netD.add(nn.LeakyReLU(0.2))\n",
    "        # state size. (ndf*2) x 16 x 16\n",
    "        netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))\n",
    "        netD.add(nn.BatchNorm())\n",
    "        netD.add(nn.LeakyReLU(0.2))\n",
    "        # state size. (ndf*4) x 8 x 8\n",
    "        netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))\n",
    "        netD.add(nn.BatchNorm())\n",
    "        netD.add(nn.LeakyReLU(0.2))\n",
    "        # state size. (ndf*8) x 4 x 4\n",
    "        netD.add(nn.Conv2D(2, 4, 1, 0, use_bias=False))\n",
    "        # state size. 2 x 1 x 1\n",
    "\n",
    "    return netD\n",
    "\n",
    "\n",
    "def get_configurations(netG, netD):\n",
    "    \"\"\"Get configurations for net\"\"\"\n",
    "    # loss\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    # initialize the generator and the discriminator\n",
    "    netG.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "    netD.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "\n",
    "    # trainer for the generator and the discriminator\n",
    "    trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "    trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "\n",
    "    return loss, trainerG, trainerD\n",
    "\n",
    "\n",
    "def ins_save(inception_score):\n",
    "    # draw the inception_score curve\n",
    "    length = len(inception_score)\n",
    "    x = np.arange(0, length)\n",
    "    plt.figure(figsize=(8.0, 6.0))\n",
    "    plt.plot(x, inception_score)\n",
    "    plt.xlabel(\"iter/100\")\n",
    "    plt.ylabel(\"inception_score\")\n",
    "    plt.savefig(\"inception_score.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def main():\n",
    "    \"\"\"Entry point to dcgan\"\"\"\n",
    "    print(\"|------- new changes!!!!!!!!!\")\n",
    "    # to get the dataset and net configuration\n",
    "    train_data, val_data = get_dataset(dataset)\n",
    "    netG = get_netG()\n",
    "    netD = get_netD()\n",
    "    loss, trainerG, trainerD = get_configurations(netG, netD)\n",
    "\n",
    "    # set labels\n",
    "    real_label = mx.nd.ones((batch_size,), ctx=ctx)\n",
    "    fake_label = mx.nd.zeros((batch_size,), ctx=ctx)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "    print('Training... ')\n",
    "    stamp = datetime.now().strftime('%Y_%m_%d-%H_%M')\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    # to metric the network\n",
    "    loss_d = []\n",
    "    loss_g = []\n",
    "    inception_score = []\n",
    "\n",
    "    for epoch in range(nepoch):\n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        for data, _ in train_data:\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real_t\n",
    "            data = data.as_in_context(ctx)\n",
    "            noise = mx.nd.random.normal(0, 1, shape=(batch_size, nz, 1, 1), ctx=ctx)\n",
    "\n",
    "            with autograd.record():\n",
    "                output = netD(data)\n",
    "                # reshape output from (batch_size, 2, 1, 1) to (batch_size, 2)\n",
    "                output = output.reshape((batch_size, 2))\n",
    "                errD_real = loss(output, real_label)\n",
    "\n",
    "            metric.update([real_label, ], [output, ])\n",
    "\n",
    "            with autograd.record():\n",
    "                fake = netG(noise)\n",
    "                output = netD(fake.detach())\n",
    "                output = output.reshape((batch_size, 2))\n",
    "                errD_fake = loss(output, fake_label)\n",
    "                errD = errD_real + errD_fake\n",
    "\n",
    "            errD.backward()\n",
    "            metric.update([fake_label,], [output,])\n",
    "\n",
    "            trainerD.step(batch_size)\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            with autograd.record():\n",
    "                output = netD(fake)\n",
    "                output = output.reshape((-1, 2))\n",
    "                errG = loss(output, real_label)\n",
    "\n",
    "            errG.backward()\n",
    "\n",
    "            trainerG.step(batch_size)\n",
    "\n",
    "            name, acc = metric.get()\n",
    "            logging.info('discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d'\n",
    "                         , mx.nd.mean(errD).asscalar(), mx.nd.mean(errG).asscalar(), acc, iter, epoch)\n",
    "            if iter % niter == 0:\n",
    "                visual('gout', fake.asnumpy(), name=os.path.join(outf, 'fake_img_iter_%d.png' % iter))\n",
    "                visual('data', data.asnumpy(), name=os.path.join(outf, 'real_img_iter_%d.png' % iter))\n",
    "                # record the metric data\n",
    "                loss_d.append(errD)\n",
    "                loss_g.append(errG)\n",
    "                if inception_score:\n",
    "                    score, _ = get_inception_score(fake)\n",
    "                    inception_score.append(score)\n",
    "\n",
    "            iter = iter + 1\n",
    "            btic = time.time()\n",
    "\n",
    "        name, acc = metric.get()\n",
    "        metric.reset()\n",
    "        logging.info('\\nbinary training acc at epoch %d: %s=%f', epoch, name, acc)\n",
    "        logging.info('time: %f', time.time() - tic)\n",
    "\n",
    "        # save check_point\n",
    "        if check_point:\n",
    "            netG.save_parameters(os.path.join(outf, 'generator_epoch_%d.params' %epoch))\n",
    "            netD.save_parameters(os.path.join(outf, 'discriminator_epoch_%d.params' % epoch))\n",
    "\n",
    "    # save parameter\n",
    "    netG.save_parameters(os.path.join(outf, 'generator.params'))\n",
    "    netD.save_parameters(os.path.join(outf, 'discriminator.params'))\n",
    "\n",
    "    # visualization the inception_score as a picture\n",
    "    if inception_score:\n",
    "        ins_save(inception_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
